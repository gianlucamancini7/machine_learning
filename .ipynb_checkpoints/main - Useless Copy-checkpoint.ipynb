{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#allows to print the dataframe nicely\n",
    "from IPython.core import display as ICD\n",
    "#!pip install plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional packages to insepct data and clean them\n",
    "import pandas as pd\n",
    "import os \n",
    "import random \n",
    "from zipfile import ZipFile\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helping functions from the implementation file\n",
    "#from proj1_helpers import load_csv_data\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Outline\n",
    "\n",
    "### Data Inspection and preparation \n",
    "In the first section the features provided were cleaned and studied; then, on the base of the scientific knowledge behind the Boson data and on the base of the features data, a method to select features was prepared. \n",
    "\n",
    "### Feature Generation\n",
    "Before actually applying regression algorithms different feature spaces were generated; in this manner the performance results obtained with the different features could be compared in the testing phase and an evaluation of the most important features could be done.\n",
    "\n",
    "### Testing\n",
    "Hence the performance of a selection of regression models was compared with different features in order to obtain the best combination according to the prediction results obtained in the Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipped files from the github repository\n",
    "data_folder='./data/'\n",
    "zip_file = ZipFile(data_folder+'all.zip')\n",
    "# zip file creates a list of files with certain properties\n",
    "zip_file.infolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training set and the testing set and creating dataframes to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to access the 'filename' property in the zipfile variable\n",
    "# and we create a dictionary of dataframe\n",
    "dfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()\n",
    "       if text_file.filename.endswith('.csv')}\n",
    "df_train=dfs['train.csv']\n",
    "df_test=dfs['test.csv']\n",
    "df_sample_submission=dfs['sample-submission.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was understanding the data structure and learning about the features we have been provided with. By studying the scientific backgroun about the experiment in the [competition description](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf), the relationship between the features was understood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to do a preliminary study of the physical meaning and relationship between the features...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first steps was taking care of the fact that certain datapoints could sometime take values not in the range of normal values. This happeneded when a data point had a value of -999.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the part of the dataset where a column would get a value not in the normal range of values\n",
    "df_train[df_train['DER_lep_eta_centrality']==-999.0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was soon discovered that almost half of the datapoints were getting values not in the range of normal values for each features. Discarting all these data was not an option hence an alternative solution was proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of number of data points to be deleted considering the feature 'DER_lep_eta_centrality' only\n",
    "df_train[df_train['DER_lep_eta_centrality']==-999.0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relationship between the values attained by each feature was present: in fact it was found that the value obtained by 'PRI_jet_num', the number of jets during the collision, was directly influencing the values of a big group of other features. In particular any time its value would be smaller or equal to one, a group of features would automatically get values out of the normal range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of relationship between 'PRI_jet_num' and 'DER_lep_eta_centrality'\n",
    "df_train.index[df_train['DER_lep_eta_centrality']==-999.0]\n",
    "df_train.index[df_train['PRI_jet_num']<=1]\n",
    "\n",
    "# checking that the indices at which -999.0 values were obtained were also the same in which 'PRI_jet_num' was less\\\n",
    "# or equal to 1\n",
    "if df_train.index[df_train['PRI_jet_num']<=1].all()==df_train.index[df_train['DER_lep_eta_centrality']==-999.0].all():\n",
    "    print ('When PRI_jet_num is less or equal to 1, DER_lep_eta_centrality gets values out of range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was then found that the value of 'PRI_jet_num' would automatically influenc the values of the following features: 'DER_deltaeta_jet_jet','DER_mass_jet_jet', 'DER_prodeta_jet_jet','DER_lep_eta_centrality', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta','PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi'. Hence the dataset was divided in two smaller dataset. One containing the features depending directly on 'PRI_jet_num' (df_train_dependent_features) and another with those not directly dependent on 'PRI_jet_num' (df_train_independent_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the independent features\n",
    "df_features_train_independent=df_train[['Id','DER_mass_MMC','DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltar_tau_lep', \n",
    "                              'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'PRI_tau_pt', \n",
    "                              'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta','PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                               'PRI_met_sumet','PRI_jet_num', 'PRI_jet_all_pt' ]]\n",
    "\n",
    "# defining the dependent features\n",
    "df_features_train_independent['PRI_jet_num']=df_features_train_independent['PRI_jet_num'].astype('float')\n",
    "\n",
    "# defining the dataframe of predictions\n",
    "prediction=df_train[['Id','Prediction']]\n",
    "\n",
    "ICD.display(df_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in df_features_train_independent.columns.values:\n",
    "    \n",
    "    index_to_drop=df_features_train_independent.index[np.abs(df_features_train_independent[column_name]-df_features_train_independent[column_name].mean()) >= (3*df_features_train_independent[column_name].std())]\n",
    "    df_features_train_independent=df_features_train_independent.drop(index_to_drop)\n",
    "    prediction=prediction.drop(index_to_drop)\n",
    "    prediction_independent=prediction\n",
    "ICD.display(len(df_features_train_independent.iloc[:,1:]))\n",
    "ICD.display(len(prediction_independent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was then noted that variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector. Variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by  the physicists of ATLAS. As it was mentioned before, it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the independent features in primitive and derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train_independent_pri=df_features_train_independent[['PRI_tau_pt', \n",
    "                              'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta','PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                               'PRI_met_sumet','PRI_jet_num', 'PRI_jet_all_pt' ]]\n",
    "df_features_train_independent_der=df_features_train_independent[['DER_mass_MMC','DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltar_tau_lep', \n",
    "                              'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to consider that by cleaning taking out all the outliers from the set of features which dependent directly on PRI_jet_num will take out all the values in which the features have invalid value (i.e. -999.0). As a matter of fact, on the one hand this action will allow us to reveal the relationship below the dominant relationship. \n",
    "\n",
    "I could see what is the proportion of data that show -999 and assign to it a value of the percentile and encapture that relationship. Then I could still keep this way calculating weigths and making and average according to the proportion of data in which they took a value of -999.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dependent features\n",
    "\n",
    "df_features_train_dependent=df_train[['Id','DER_deltaeta_jet_jet','DER_mass_jet_jet', 'DER_prodeta_jet_jet','DER_lep_eta_centrality',\n",
    "                             'PRI_jet_leading_pt', 'PRI_jet_leading_eta','PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
    "                             'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi','PRI_jet_num']]\n",
    "\n",
    "df_features_train_dependent['PRI_jet_num']=df_features_train_dependent['PRI_jet_num'].astype('float')\n",
    "\n",
    "# defining the dataframe of predictions\n",
    "prediction=df_train[['Id','Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in df_features_train_dependent.columns.values:\n",
    "    \n",
    "    index_to_drop_invalid=df_features_train_dependent.index[df_features_train_dependent['PRI_jet_num']<=1]\n",
    "    df_features_train_dependent=df_features_train_dependent.drop(index_to_drop_invalid)\n",
    "    prediction=prediction.drop(index_to_drop_invalid)\n",
    "    prediction_dependent=prediction\n",
    "    \n",
    "    index_to_drop=df_features_train_dependent.index[np.abs(df_features_train_dependent[column_name]-df_features_train_dependent[column_name].mean()) >= (3*df_features_train_dependent[column_name].std())]\n",
    "    df_features_train_dependent=df_features_train_dependent.drop(index_to_drop)\n",
    "    prediction=prediction.drop(index_to_drop)\n",
    "    prediction_dependent=prediction\n",
    "    \n",
    "ICD.display(len(df_features_train_dependent.iloc[:,1:]))\n",
    "ICD.display(len(prediction_dependent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the independent dataframe of variables shouldn't have invalid values since the columns which would get invalid values which are dependent by PRI_jet_num have been removed, the column of DER mass MMC can still have invalid values. In fact the estimated mass mH of the Higgs boson candidate, obtained through a prob- abilistic phase space integration (may be undefined if the topology of the event is too far from the expected topology)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute with the median the values which are still -999.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in df_features_train_independent.columns.values:\n",
    "    \n",
    "    df_features_train_independent[column_name][df_features_train_independent[column_name]==-999.0]=np.median(df_features_train_independent[column_name])\n",
    "\n",
    "df_features_train_dependent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating of toy data set\n",
    "df_toy=df_train\n",
    "for column_name in df_toy.columns.values[2:]:\n",
    "    \n",
    "    df_toy[column_name][df_toy[column_name]==-999.0]=np.median(df_toy[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the correlation between the dependent and independent features were found to have an idea of the linear dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train_independent.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train_dependent.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the independent set of features were firstly considered in order to carry out a principal component analysis. In orde to have an idea of the distribution of the features when they take the value of 'b' or 's', each feature has been normalized and its density distribution has been plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'b': 'rgb(31, 119, 180)', \n",
    "          's': 'rgb(255, 127, 14)'}\n",
    "features = df_features_train_independent.iloc[:,1:].columns.values\n",
    "\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "for feature in features:\n",
    "    \n",
    "    # Subset for boson and not boson\n",
    "    subset_boson = df_features_train_independent[prediction_independent['Prediction'] == 'b']\n",
    "    \n",
    "    subset_non_boson = df_features_train_independent[prediction_independent['Prediction'] == 's']\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(standardize_personal(subset_boson[feature]), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = feature)\n",
    "#     print (feature)\n",
    "#     print()\n",
    "#     print ('non standardized distribution of values')\n",
    "#     print (subset_boson[feature].describe(include=None))\n",
    "    \n",
    "# Plot formatting\n",
    "\n",
    "plt.legend(prop={'size': 10}, title = 'Features')\n",
    "plt.title('Density plot of rows with boson')\n",
    "plt.xlabel('Standardized axes')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the distribution of each feature\n",
    "subset_boson = df_features_train_independent[prediction_independent['Prediction'] == 'b']\n",
    "colors = {'b': 'rgb(31, 119, 180)', \n",
    "          's': 'rgb(255, 127, 14)'}\n",
    "\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "sns.distplot(standardize_personal(subset_boson['PRI_jet_num']), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = feature)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title = 'Features')\n",
    "plt.title('Density plot of rows with boson')\n",
    "plt.xlabel('Standardized axes')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly each feature was standardized since the units were different. It can be seen that the distribution of certain features when they take the value of 'b'  are particularly interesting: PRI_jet_num, can take only three discrete values shows three spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = {'b': 'rgb(31, 119, 180)', \n",
    "          's': 'rgb(255, 127, 14)'}\n",
    "features = df_features_train_independent.iloc[:,1:].columns.values\n",
    "\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "for feature in features:\n",
    "    \n",
    "    # Subset for boson and not boson\n",
    "    subset_boson = df_features_train_independent[prediction_independent['Prediction'] == 'b']\n",
    "    \n",
    "    subset_non_boson = df_features_train_independent[prediction_independent['Prediction'] == 's']\n",
    "    \n",
    "    # Draw the density plot\n",
    "    sns.distplot(standardize_personal(subset_non_boson[feature]), hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = feature)\n",
    "#     print (feature)\n",
    "#     print()\n",
    "#     print ('non standardized distribution of values')\n",
    "#     print (subset_non_boson[feature].describe(include=None))\n",
    "    \n",
    "# Plot formatting\n",
    "\n",
    "plt.legend(prop={'size': 10}, title = 'Features')\n",
    "plt.title('Density plot of rows with boson')\n",
    "plt.xlabel('Standardized axes')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, one can see that there are certain features whose variance is much bigger than otherswhen they take a value of 'b' or 's'. There are other features, however, such as DER_mass_MMC whose value distribution is extremely skewed when they take a value of 'b' or 's'. By comparing the different types of distribution of the features when having a value of 'b' or 's', one can see which ones are mostly difference to have an understanding of which features are most important in determining 'b' or 's'. ???????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the arrays and discarting the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_features_train_independent.iloc[:,1:].columns.values\n",
    "df_features_train_independent_std=pd.DataFrame()\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    df_features_train_independent_std[feature]=standardize_personal(df_features_train_independent[feature])\n",
    "df_features_train_independent_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the covariance matrix in order to the find its eigenvectors and eigenvalues which will be an indication of what are the most principal components since they will tell how much and in which direction the data mostly vary. This is in this case needed since the data have different units and meanings. It can happen, however that by standardizing the features some relationships will be lost. In fact, if on the one side to find the principal components standardization is needed to take account of the different orders of magnitude, it is also important to mention that this analysis is strongly subjected to variations of the features; for example, if a feature is multiplied by a constant, and then the principal components analysis is done, the results are going to be different from those done with the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_features_train_independent_std=np.cov(df_features_train_independent_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_df_features_train_independent_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the eigendecomposition on the covariance matrix Σ which is a d×d matrix where each element represents the covariance between two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular value decomposition. The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u,s,v=np.linalg.svd(df_features_train_independent_std[:20000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Everything ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top k\n",
    " eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "trace1 = Bar(\n",
    "        x=['PC %s' %i for i in range(1,20)],\n",
    "        y=var_exp,\n",
    "        showlegend=False)\n",
    "\n",
    "trace2 = Scatter(\n",
    "        x=['PC %s' %i for i in range(1,20)], \n",
    "        y=cum_var_exp,\n",
    "        name='cumulative explained variance')\n",
    "\n",
    "data = Data([trace1, trace2])\n",
    "\n",
    "layout=Layout(\n",
    "        yaxis=YAxis(title='Explained variance in percent'),\n",
    "        title='Explained variance by different principal components')\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the top components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs[0][1].reshape(20,1)\n",
    "number_features=20\n",
    "#decide how many principal components i get\n",
    "number_pa=15\n",
    "#define matrix to be filled in\n",
    "matrix_w=np.ones((number_features, number_pa))\n",
    "for i in range(number_pa):\n",
    "    matrix_w[:,i] = eig_pairs[i][1]\n",
    "matrix_w[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_train_independent_std_transf = df_features_train_independent_std.dot(matrix_w)\n",
    "df_features_train_independent_std_transf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=15)\n",
    "df_features_train_independent_std_transf_sickit=pca.fit_transform(df_features_train_independent_std)\n",
    "df_features_train_independent_std_transf_sickit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers deletion for independent set and prediction set in order not to mix rows once the outliers are deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the arrays for dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for independent variable\n",
    "yb_independent, input_data_independent, ids_independent=np.array(prediction_independent['Prediction']), np.array(df_features_train_independent.iloc[:,1:]), np.array(df_features_train_independent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for dependent variable\n",
    "yb_dependent, input_data_dependent, ids_dependent=np.array(prediction_dependent['Prediction']), np.array(df_features_train_dependent.iloc[:,1:]), np.array(df_features_train_dependent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for primitive variable\n",
    "yb_independent_pri, input_data_independent_pri, ids_dependent_pri=np.array(prediction_independent['Prediction']), np.array(df_features_train_independent_pri.iloc[:,1:]), np.array(df_features_train_independent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for primitive variable\n",
    "yb_independent_der, input_data_independent_der, ids_dependent_der=np.array(prediction_independent['Prediction']), np.array(df_features_train_independent_der.iloc[:,1:]), np.array(df_features_train_independent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for independent variable\n",
    "yb_independent_pa, input_data_independent_pa, ids_independent=np.array(prediction_independent['Prediction']), np.array(df_features_train_independent_std_transf), np.array(df_features_train_independent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for independent variable\n",
    "yb_independent_pa_scikit, input_data_independent_pa_scikit, ids_independent_scikit=np.array(prediction_independent['Prediction']), np.array(df_features_train_independent_std_transf_sickit), np.array(df_features_train_independent['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the indices from the dataframes and set up the dataframes for independent variable\n",
    "yb_toy, input_data_toy, ids_toy=np.array(df_toy['Prediction']), np.array(df_toy.iloc[:,2:]), np.array(df_toy['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform yb into numerical values\n",
    "yb_independent[np.where(yb_independent=='b')] = 1.\n",
    "yb_independent[np.where(yb_independent=='s')] = 0.\n",
    "yb_independent=yb_independent.astype('float')\n",
    "\n",
    "yb_independent_pri[np.where(yb_independent_pri=='b')] = -1.\n",
    "yb_independent_pri[np.where(yb_independent_pri=='s')] = 1.\n",
    "yb_independent_pri=yb_independent_pri.astype('float')\n",
    "\n",
    "yb_independent_der[np.where(yb_independent_der=='b')] = -1.\n",
    "yb_independent_der[np.where(yb_independent_der=='s')] = 1.\n",
    "yb_independent_der=yb_independent_der.astype('float')\n",
    "\n",
    "yb_dependent[np.where(yb_dependent=='b')] = -1.\n",
    "yb_dependent[np.where(yb_dependent=='s')] = 1.\n",
    "yb_dependent=yb_dependent.astype('float')\n",
    "\n",
    "yb_independent_pa[np.where(yb_independent_pa=='b')] = 1.\n",
    "yb_independent_pa[np.where(yb_independent_pa=='s')] = 0.\n",
    "yb_independent_pa=yb_independent_pa.astype('float')\n",
    "\n",
    "\n",
    "yb_toy[np.where(yb_toy=='b')] = 1.\n",
    "yb_toy[np.where(yb_toy=='s')] = 0.\n",
    "yb_toy=yb_toy.astype('float')\n",
    "\n",
    "\n",
    "yb_independent_pa_scikit[np.where(yb_independent_pa_scikit=='b')] = 1.\n",
    "yb_independent_pa_scikit[np.where(yb_independent_pa_scikit=='s')] = 0.\n",
    "yb_independent_pa_scikit=yb_independent_pa_scikit.astype('float')\n",
    "\n",
    "\n",
    "#Cut dataframe for fast testing\n",
    "# lines_cut=200000\n",
    "# tx=input_data[:lines_cut]\n",
    "# y=yb[:lines_cut]\n",
    "# y.shape\n",
    "\n",
    "\n",
    "y_independent=yb_independent\n",
    "y_dependent=yb_dependent\n",
    "y_independent_pri=yb_independent_pri\n",
    "y_independent_der=yb_independent_der\n",
    "y_independent_pa=yb_independent_pa\n",
    "y_toy=yb_toy\n",
    "y_independent_pa_scikit=yb_independent_pa_scikit\n",
    "\n",
    "tx_independent=input_data_independent\n",
    "tx_dependent=input_data_dependent\n",
    "tx_independent_pri=input_data_independent_pri\n",
    "tx_independent_der=input_data_independent_der\n",
    "tx_independent_pa=input_data_independent_pa\n",
    "tx_toy=input_data_toy\n",
    "tx_independent_pa_scikit=input_data_independent_pa_scikit\n",
    "#input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 30\n",
    "gamma = 0.1\n",
    "batch_size = 2000\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tx.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_independent=least_squares(y_independent,tx_independent)\n",
    "loss_independent=compute_mse(y_independent, tx_independent, w_independent)\n",
    "print('Weigths: ',w_independent,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_independent_pri=least_squares(y_independent_pri,tx_independent_pri)\n",
    "loss_independent_pri=compute_mse(y_independent_pri, tx_independent_pri, w_independent_pri)\n",
    "print('Weigths: ',w_independent_pri,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent_pri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_independent_der=least_squares(y_independent_der,tx_independent_der)\n",
    "loss_independent_der=compute_mse(y_independent_der, tx_independent_der, w_independent_der)\n",
    "print('Weigths: ',w_independent_der,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent_der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dependent=least_squares(y_dependent,tx_dependent)\n",
    "loss_dependent=compute_mse(y_dependent, tx_dependent, w_dependent)\n",
    "print('Weigths: ',w_dependent,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_dependent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_independent_pa=least_squares(y_independent_pa,tx_independent_pa)\n",
    "loss_independent_pa=compute_mse(y_independent_pa, tx_independent_pa, w_independent_pa)\n",
    "print('Weigths: ',w_independent_pa,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent_pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_independent_scikit=least_squares(y_independent_pa_scikit,tx_independent_pa_scikit)\n",
    "loss_independent_scikit=compute_mse(y_independent_pa_scikit, tx_independent_pa_scikit, w_independent_scikit)\n",
    "print('Weigths: ',w_independent_scikit,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent_scikit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_toy=least_squares(y_toy,tx_toy)\n",
    "loss_toy=compute_mse(y_toy, tx_toy, w_toy)\n",
    "print('Weigths: ',w_toy,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least square polynomial expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, order):\n",
    "    \"https://stackoverflow.com/questions/11723779/2d-numpy-power-for-polynomial-expansion\"\n",
    "    \"\"\"For each row of ndarray x, the polynomial expansions are computed, i.e\n",
    "    for row [x1, x2] and order 2, the following row of the result matrix is\n",
    "    computed: [1, x1, x1**2, x2, x1*x2, x1**2*x2, x2**2, x1*x2**2, x1**2*x2**2]\"\"\"\n",
    "    x = np.asarray(x).T[np.newaxis]\n",
    "    n = x.shape[1]\n",
    "    power_matrix = np.tile(np.arange(order + 1), (n, 1)).T[..., np.newaxis]\n",
    "    X = np.power(x, power_matrix)\n",
    "    I = np.indices((order + 1, ) * n).reshape((n, (order + 1) ** n)).T\n",
    "    F = np.product(np.diagonal(X[I], 0, 1, 2), axis=2)\n",
    "    return F.T\n",
    "\n",
    "aa=np.array([[1,2,3,4,7,5],[5,9,13,7,8,7]])\n",
    "gg=np.array([2,3])\n",
    "\n",
    "tx_independent_exp=polynomial_features(aa, 2)\n",
    "w_independent_exp=least_squares(gg,tx_independent_exp)\n",
    "loss_independent_exp=compute_mse(gg, tx_independent_exp, w_independent_exp)\n",
    "print('Weigths: ',w_independent_exp,\"\\n\\n\",'RMSE: ',np.sqrt(2*loss_independent_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 8\n",
    "lambdas = np.logspace(-5, 3, 15)\n",
    "\n",
    "# Initialization\n",
    "k_indices=build_k_indices(yb_independent_pa, k_fold, seed)\n",
    "cross_rmse_train=[]\n",
    "cross_rmse_test=[]\n",
    "wsi_train_avg=[]\n",
    "for lambda_ in lambdas:\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    wsi_train_lst=[]\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te,wsi_train=cross_validation_ridge(y_independent_pa, tx_independent_pa, k_indices, k, lambda_)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "        wsi_train_lst.append(wsi_train)\n",
    "    cross_rmse_train.append(np.mean(rmse_tr))\n",
    "    cross_rmse_test.append(np.mean(rmse_te))\n",
    "    wsi_train_avg.append(np.mean(wsi_train))\n",
    "cross_validation_visualization(lambdas, cross_rmse_train, cross_rmse_test)    \n",
    "print(wsi_train_avg[np.argmin(cross_rmse_test)])\n",
    "print(np.min(cross_rmse_test))\n",
    "wsi_train_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the lambda that perform better and use it to do the regression\n",
    "lambda_final_index=np.where(wsi_train_avg==wsi_train_avg[np.argmin(cross_rmse_test)])[0][0]\n",
    "loss_tr, loss_te,wsi_ridge=cross_validation_ridge(y_independent_pa, tx_independent_pa, k_indices, k, lambdas[lambda_final_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for both the dependent and the independent set and merging of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 8\n",
    "\n",
    "# Initialization\n",
    "k_indices=build_k_indices(y_independent_pa, k_fold, seed)\n",
    "\n",
    "cross_rmse_train=[]\n",
    "cross_rmse_test=[]\n",
    "wsi_train_lst=[]\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_tr, loss_te,wsi_train=cross_validation_least_squares(y_independent_pa, tx_independent_pa, k_indices, k)\n",
    "    cross_rmse_train.append(loss_tr)\n",
    "    cross_rmse_test.append(loss_te)\n",
    "    wsi_train_lst.append(wsi_train)\n",
    "cross_rmse_train=np.average(cross_rmse_train)\n",
    "cross_rmse_test=np.average(cross_rmse_test)\n",
    "wsi_train_lst=np.average(wsi_train_lst,axis=0)\n",
    "w_independent=wsi_train_lst\n",
    "print(wsi_train_lst)\n",
    "print(cross_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "seed = 1\n",
    "k_fold = 8\n",
    "\n",
    "# Initialization\n",
    "k_indices=build_k_indices(y_dependent, k_fold, seed)\n",
    "\n",
    "cross_rmse_train=[]\n",
    "cross_rmse_test=[]\n",
    "wsi_train_lst=[]\n",
    "\n",
    "for k in range(k_fold):\n",
    "    loss_tr, loss_te,wsi_train=cross_validation_least_squares(y_dependent, tx_dependent, k_indices, k)\n",
    "    cross_rmse_train.append(loss_tr)\n",
    "    cross_rmse_test.append(loss_te)\n",
    "    wsi_train_lst.append(wsi_train)\n",
    "cross_rmse_train=np.average(cross_rmse_train)\n",
    "cross_rmse_test=np.average(cross_rmse_test)\n",
    "wsi_train_lst=np.average(wsi_train_lst,axis=0)\n",
    "w_dependent=wsi_train_lst\n",
    "print(wsi_train_lst)\n",
    "print(cross_rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the weights given by the two set of dataframes\n",
    "w=pd.Series(np.ones((df_test.shape[1]-2)))\n",
    "total_features=pd.Series(df_train.iloc[:,2:].columns.values)\n",
    "independent_features=pd.Series(df_features_train_independent.iloc[:,1:].columns.values)\n",
    "dependent_features=pd.Series(df_features_train_dependent.iloc[:,1:].columns.values)\n",
    "\n",
    "w[total_features.index[total_features.isin(independent_features)]]=w_independent\n",
    "w[total_features.index[total_features.isin(dependent_features)]]=w_dependent\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "gamma = 0.01\n",
    "lambda_ = 0.01\n",
    "threshold = 1e-8\n",
    "points=10000\n",
    "tx_log=tx_toy\n",
    "losses = []\n",
    "y=y_toy[:, np.newaxis]\n",
    "y_log=y\n",
    "w = np.zeros((tx_log[:points].shape[1], 1))\n",
    "loss=1\n",
    "\n",
    "# start the logistic regression\n",
    "for iter in range(max_iter):\n",
    "    if loss<0:\n",
    "        break\n",
    "    \n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_penalized_gradient(y_log[:points], tx_log[:points], w, gamma, lambda_)\n",
    "    # log info\n",
    "\n",
    "\n",
    "    if iter % 10 == 0:\n",
    "        print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "\n",
    "    \n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "         break\n",
    "    \n",
    "# # visualization\n",
    "# visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "print(\"loss={l}\".format(l=calculate_loss(y_log[:points], tx_log[:points], w)))\n",
    "#print('w',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly the test data have to be formatted, cleaned and features have to be generated in the same manner the train set was treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1=df_test[['Id','DER_mass_MMC','DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltar_tau_lep', \n",
    "                              'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'PRI_tau_pt', \n",
    "                              'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta','PRI_lep_phi', 'PRI_met', 'PRI_met_phi', \n",
    "                               'PRI_met_sumet','PRI_jet_num', 'PRI_jet_all_pt' ]]\n",
    "# defining the dependent features\n",
    "df_test_1['PRI_jet_num']=df_test_1['PRI_jet_num'].astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in df_test_1.columns.values:\n",
    "    \n",
    "    index_to_change=df_test_1.index[np.abs(df_test_1[column_name]-df_test_1[column_name].mean()) >= (3*df_test_1[column_name].std())]\n",
    "    df_test_1[column_name][index_to_change]=np.median(df_test_1[column_name])\n",
    "    \n",
    "for column_name in df_test_1.columns.values:\n",
    "    \n",
    "    df_test_1[column_name][df_test_1[column_name]==-999.0]=np.median(df_test_1[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_test_1.iloc[:,1:].columns.values\n",
    "df_test_1_std=pd.DataFrame()\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    df_test_1_std[feature]=standardize_personal(df_test_1[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_df_test_1_std=np.cov(df_test_1_std.T)\n",
    "eig_vals_test, eig_vecs_test = np.linalg.eig(cov_df_test_1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs_test = [(np.abs(eig_vals_test[i]), eig_vecs_test[:,i]) for i in range(len(eig_vals_test))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs_test.sort()\n",
    "eig_pairs_test.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_pairs_test[0][1].reshape(20,1)\n",
    "number_features=20\n",
    "#decide how many principal components i get\n",
    "number_pa=15\n",
    "#define matrix to be filled in\n",
    "matrix_w_test=np.ones((number_features, number_pa))\n",
    "\n",
    "for i in range(number_pa):\n",
    "    matrix_w_test[:,i] = eig_pairs_test[i][1]\n",
    "\n",
    "df_test_1_std_transf = df_test_1_std.dot(matrix_w_test)\n",
    "df_test_1_std_transf.head()\n",
    "matrix_w_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1_std_transf=df_test_1_std.dot(matrix_w)\n",
    "df_test_1_std_transf.head()\n",
    "matrix_w[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tx_test=df_test.iloc[:,2:]\n",
    "tx_test=df_test_1_std_transf\n",
    "y_pred=predict_labels(wsi_train_lst,np.array(tx_test))\n",
    "create_csv_submission(df_test['Id'], y_pred, 'trial_least_square_pa_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting or underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other stuff\n",
    "Build Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
